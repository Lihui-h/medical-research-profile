# src/crawlers/social_crawlers/baidu_crawler.py
import os
import sys
import random
import time
import csv
import logging
import marshal
from urllib.parse import quote
from pathlib import Path
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import requests
import json
from webdriver_manager.chrome import ChromeDriverManager
from requests.adapters import HTTPAdapter
from supabase import create_client, Client
from dotenv import load_dotenv
from src.utils.api_client import OxylabsScraper  # æ–°å¢å¯¼å…¥
from src.utils.keyword_generator import KeywordGenerator  # æ–°å¢å¯¼å…¥

# é…ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler("logs/tieba_crawler.log", encoding='utf-8'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

class WeightedSentimentAnalyzer:
    def __init__(self):
        # åŠ è½½è¯åº“ï¼ˆå¯ä»æ–‡ä»¶è¯»å–ï¼‰
        self.negative_weights = {
            "åŒ»ç–—äº‹æ•…": -5,
            "æ²»æ®‹": -5,
            "åƒåœ¾": -4,
            "å·®è¯„": -4,
            "ç„¦è™‘": -3,
            "ä¸æ˜çœŸç›¸": -2,
            "ä¸ä¸“ä¸š": -2,
            "ä¸è´Ÿè´£": -2,
            "ç”Ÿæ°”": -2,
            "éª—": -3,
            "å¯è€»": -3,
        }
        self.positive_weights = {
            "ä¸“ä¸š": +3,
            "è´Ÿè´£": +2,
            "ç»éªŒä¸°å¯Œ": +2,
            "å¥½åŒ»ç”Ÿ": +3,
            "å¥½å¼€å¿ƒ": +3,
            "æœ‰çˆ±å¿ƒ": +2,
        }
        
        self.thresholds = (-3, 3)  # (negative_threshold, positive_threshold)

    def calculate_score(self, text):
        """åŠ æƒè¯„åˆ†ç®—æ³•"""
        score = 0
        # è´Ÿé¢è¯æ£€æµ‹
        for word, weight in self.negative_weights.items():
            if word in text:
                score += weight * text.count(word)  # æŒ‰å‡ºç°æ¬¡æ•°ç´¯åŠ 
        # æ­£é¢è¯æ£€æµ‹
        for word, weight in self.positive_weights.items():
            if word in text:
                score += weight * text.count(word)
        return score

    def analyze(self, text):
        """æƒ…æ„Ÿåˆ†ç±»"""
        score = self.calculate_score(text)
        if score <= self.thresholds[0]:
            return ("negative", score)
        elif score >= self.thresholds[1]:
            return ("positive", score)
        else:
            return ("neutral", score)

class TiebaSpider:
    """ç™¾åº¦è´´å§çˆ¬è™«ï¼ˆå¢å¼ºä»£ç†ç¨³å®šæ€§ç‰ˆï¼‰"""
    #åˆå§‹åŒ–æ–¹æ³•
    def __init__(self, kw='æµ™æ±Ÿçœä¸­åŒ»é™¢', max_page=2, delay=10):
        # åˆå§‹åŒ–å‚æ•°
        self.keyword_tool = KeywordGenerator()  # æ–°å¢
        self.final_keywords = self.keyword_tool.get_encoded_keywords()  # è·å–ç¼–ç åçš„å…³é”®è¯
        self.logger = logging.getLogger(self.__class__.__name__)
        self.keyword = kw
        self.api_client = OxylabsScraper()
        self.base_url = "https://tieba.baidu.com"
        self.data = []
        # åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨ï¼ˆæ›¿æ¢åŸsnownlpç›¸å…³ä»£ç ï¼‰
        self.sentiment_analyzer = WeightedSentimentAnalyzer()
        
        
        load_dotenv()
        self.supabase: Client = create_client(
            os.getenv("SUPABASE_URL"),
            os.getenv("SUPABASE_KEY") 
        )

    #æ ¸å¿ƒæ–¹æ³•
    def run(self):
        """æ–°ç‰ˆä¸»è¿è¡Œé€»è¾‘"""
        self.logger.info("ğŸš€ å¯åŠ¨è´´å§æ•°æ®é‡‡é›†å¼•æ“...")
        search_urls = self.generate_search_urls()
        
        total_count = 0
        for idx, url in enumerate(search_urls, 1):
            self.logger.info(f"â–· å¤„ç†URL [{idx}/{len(search_urls)}]: {url[:60]}...")

            # è°ƒç”¨APIè·å–æ•°æ®
            api_response = self.api_client.fetch_page(url)
            if not api_response.get("results"):
                continue

            # è§£æåˆ—è¡¨é¡µ
            self.parse_list_page(api_response["results"][0]["content"])
            if not self.data:
                self.logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•å¸–å­æ•°æ®")
                continue
            
            # ç«‹å³å­˜å‚¨å½“å‰æ‰¹æ¬¡æ•°æ®
            self._save_data()
            total_count += len(self.data)
            self.logger.info(f"âœ… å·²è·å– {len(self.data)} æ¡å¸–å­æ•°æ® | ç´¯è®¡: {total_count} æ¡")
            self.data.clear()  # æ¸…ç©ºå½“å‰æ‰¹æ¬¡æ•°æ®

            # åˆç†çš„å»¶è¿Ÿ
            time.sleep(random.uniform(1.5, 3.5))  

        # æœ€ç»ˆæ—¥è®°
        self.logger.info(f"ğŸ å…¨éƒ¨é‡‡é›†å®Œæˆ | æ€»æ•°æ®é‡: {total_count} æ¡")

    
    def _run_static_mode(self):
        """é™æ€è§£ææ¨¡å¼ä¸“ç”¨æµç¨‹"""
        logger.info("ğŸ“„ è¿›å…¥é™æ€è§£ææ¨¡å¼")
        for page in range(1, self.max_page + 1):
            if not self.parse_list_page(page):
                break
            time.sleep(self.delay + random.uniform(1, 3))

        # è·å–è¯¦æƒ…
        logger.info(f"ğŸ” å¼€å§‹å¤„ç† {len(self.data)} æ¡å¸–å­è¯¦æƒ…")
        for idx, item in enumerate(self.data, 1):
            self.get_post_detail(item)
            time.sleep(random.uniform(0.8, 2.2))

    def parse_list_page(self, html: str):
        """è§£æè´´å§åˆ—è¡¨é¡µï¼ˆé€‚é…æ–°ç‰ˆé¡µé¢ç»“æ„ï¼‰"""
        soup = BeautifulSoup(html, "html.parser")
        post_list = soup.select('div.s_post')

        for post in post_list:
            item = {}
            try:
                # ===== æ ‡é¢˜ä¸é“¾æ¥ =====
                title_elem = post.select_one('span.p_title a.bluelink')
                item['title'] = title_elem.text.strip() if title_elem else "æ— æ ‡é¢˜"

                # æ„å»ºå®Œæ•´é“¾æ¥ï¼ˆéœ€å¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
                if title_elem and title_elem.has_attr('href'):
                    item['detail_url'] = f"https://tieba.baidu.com{title_elem['href'].split('?')[0]}"  # å»é™¤å‚æ•°ä¿ç•™çº¯å‡€URL
                else:
                    item['detail_url'] = "é“¾æ¥æ— æ•ˆ"
                
                # ===== æ­£æ–‡å†…å®¹ =====
                content_elem = post.select_one('div.p_content')
                raw_content = content_elem.text.strip() if content_elem else "å†…å®¹è§£æå¤±è´¥"
                item['content'] = raw_content[:500]  # é™åˆ¶é•¿åº¦é˜²æ­¢è¶…å­—æ®µé™åˆ¶

                # ===== è´´å§ä¿¡æ¯ =====
                forum_elem = post.select_one('a.p_forum font.p_violet')
                item['forum'] = forum_elem.text.strip() if forum_elem else "æœªçŸ¥è´´å§"

                # ===== ä½œè€…ä¿¡æ¯ =====
                author_elem = post.select_one('a[href^="/home/main"] font.p_violet')  # ç²¾å‡†å®šä½ä½œè€…
                item['author'] = author_elem.text.strip() if author_elem else "åŒ¿åç”¨æˆ·"

                # ===== æ—¶é—´ä¿¡æ¯ =====
                date_elem = post.select_one('font.p_date')
                item['raw_post_time'] = date_elem.text.strip() if date_elem else "æ—¶é—´æœªæ ‡æ³¨"

                item.update({
                    #å›ºå®šå­—æ®µ
                    'source': 'baidu_tieba',
                    'institution_name': 'æµ™æ±Ÿçœä¸­åŒ»é™¢',
                    'org_code': 'zjszyy',
                    'user_id': os.getenv("SUPABASE_USER_UUID")
                })

                sentiment_result = self.sentiment_analyzer.analyze(raw_content)
                # å¦‚æœè¿”å›å…ƒç»„ï¼ˆæ ‡ç­¾ï¼Œåˆ†æ•°ï¼‰
                item['sentiment'] = sentiment_result[0]  # å–æƒ…æ„Ÿæ ‡ç­¾
                item['sentiment_score'] = sentiment_result[1]  # å–æƒ…æ„Ÿåˆ†æ•°

                # ==== æ–°å¢æœ‰æ•ˆæ€§æ ¡éªŒ ====
                required_keyword = 'æµ™æ±Ÿçœä¸­åŒ»é™¢'
                valid_check = [
                    item.get('detail_url') and item['detail_url'] != "é“¾æ¥æ— æ•ˆ",  # æ£€æŸ¥æœ‰æ•ˆé“¾æ¥
                    len(item.get('content', '')) > 10,                         # å†…å®¹é•¿åº¦é™åˆ¶
                    required_keyword in item.get('title', '') or required_keyword in item.get('content', '')  # å…³é”®è¯åŒ¹é…
                ]
                if not all(valid_check):
                    log_msg = f"è·³è¿‡æ— æ•ˆæ•°æ® | åŸå› ï¼š"
                    reasons = []
                    if not valid_check[0]:
                        reasons.append("æ— æ•ˆé“¾æ¥")
                    if not valid_check[1]:
                        reasons.append(f"å†…å®¹è¿‡çŸ­ï¼ˆ{len(item.get('content',''))}å­—ï¼‰") 
                    if not valid_check[2]:
                        reasons.append(f"æœªåŒ…å«å…³é”®è¯'{required_keyword}'")

                    self.logger.warning(log_msg + "ï¼Œ".join(reasons))
                    continue
                
                # è¿½åŠ åˆ°æ•°æ®åˆ—è¡¨
                self.data.append(item)
            except Exception as e:
                self.logger.error(f"è§£æå¼‚å¸¸: {str(e)}")
                # ä¿å­˜é”™è¯¯æ ·æœ¬ç”¨äºè°ƒè¯•
                with open("error_post.html", "w", encoding="utf-8") as f:
                    f.write(str(post))
            
        if not self.data:
            self.logger.warning("âš ï¸ æœªè§£æåˆ°ä»»ä½•å¸–å­æ•°æ®ï¼Œè¯·æ£€æŸ¥HTMLç»“æ„æˆ–é€‰æ‹©å™¨")

    def crawl_details(self):
        """æ‰¹é‡è·å–è¯¦æƒ…é¡µ"""
        for idx, item in enumerate(self.data, 1):
            detail_response = self.api_client.fetch_page(item['detail_url'])
            if detail_html := detail_response.get("results", [{}])[0].get("content"):
                self.parse_detail(item, detail_html)
            self.logger.info(f"è¿›åº¦: {idx}/{len(self.data)}")
            time.sleep(1.5)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡

    def parse_detail(self, item: dict, html: str):
        """è§£æè¯¦æƒ…é¡µ"""
        soup = BeautifulSoup(html, "html.parser")
        content_div = soup.find('div', class_='d_post_content')
        item['content'] = content_div.text.strip() if content_div else ''

    def get_post_detail(self, item):
        """é€šè¿‡ API è·å–è¯¦æƒ…é¡µå†…å®¹"""
        try:
            # è°ƒç”¨ API è·å–è¯¦æƒ…é¡µ
            detail_response = self.api_client.fetch_page(item['detail_url'])

            # æ ¡éªŒ API å“åº”
            if not detail_response.get("results"):
                logger.error(f"API å“åº”å¼‚å¸¸: {detail_response}")
                return
            
            # æå– HTML å†…å®¹
            detail_html = detail_response["results"][0].get("content", "")
            if not detail_html:
                logger.warning(f"è¯¦æƒ…é¡µå†…å®¹ä¸ºç©º: {item['detail_url']}")
                return
            
            # è§£æå†…å®¹
            soup = BeautifulSoup(detail_html, "html.parser")
            content_div = soup.find('div', class_='d_post_content')
            item['content'] = content_div.text.strip() if content_div else ''
            
            
            # éšæœºå»¶è¿Ÿï¼ˆ1-3ç§’ï¼‰
            time.sleep(random.uniform(1, 3))
            
        except Exception as e:
            logger.error(f"è¯¦æƒ…é¡µè·å–å¤±è´¥: {str(e)}")

    #å­˜å‚¨æ–¹æ³•
    def _save_data(self):
        """ç›´æ¥å­˜å‚¨åˆ°Supabase"""
        if not self.data:
            self.logger.warning("âš ï¸ æ— æœ‰æ•ˆæ•°æ®å¯å­˜å‚¨")
            return
        
        try:
            # æ‰¹é‡æ’å…¥å¹¶å»é‡
            response = self.supabase.table('posts').upsert(
                self.data,
                on_conflict='detail_url'
            ).execute()
            
            if len(response.data) > 0:
                self.logger.info(f"âœ… æˆåŠŸå†™å…¥ {len(response.data)} æ¡æ•°æ®")
            else:
                self.logger.warning("âš ï¸ æ— æ–°æ•°æ®å†™å…¥")
        except Exception as e:
            self.logger.error(f"å­˜å‚¨å¤±è´¥: {str(e)}")

    #å·¥å…·æ–¹æ³•
    def _anonymize_data(self, item):
        """æ•°æ®åŒ¿ååŒ–å¤„ç†"""
        # ç§»é™¤ç”¨æˆ·IDå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        item.pop('user_id', None)
        
        # æ—¶é—´æ¨¡ç³Šå¤„ç†ï¼ˆä¿ç•™åˆ°å¤©ï¼‰
        if 'publish_time' in item and isinstance(item['publish_time'], str):
            item['publish_time'] = item['publish_time'].split(' ')[0]
        
        # æ•æ„Ÿè¯æ›¿æ¢
        sensitive_terms = {'è‰¾æ»‹ç—…': 'æŸä¼ æŸ“ç—…', 'ä¹™è‚': 'æŸç—…æ¯’æ€§è‚ç‚'}
        for term, replacement in sensitive_terms.items():
            item['content'] = item['content'].replace(term, replacement)
        return item

    def generate_search_urls(self):
        """ç”Ÿæˆå¸¦åˆ†é¡µçš„æœç´¢URL"""
        base_url = "https://tieba.baidu.com/f/search/res"
        urls = []
        for keyword in self.keyword_tool.generate():  # è·å–åŸå§‹å…³é”®è¯
            # å°†å…³é”®è¯è½¬ä¸ºGBKç¼–ç 
            try:
                gbk_bytes = keyword.encode('gbk', errors='strict')
            except UnicodeEncodeError:
                self.logger.error(f"å…³é”®è¯'{keyword}'æ— æ³•ç”¨GBKç¼–ç ï¼Œå·²è·³è¿‡")
                continue

            encoded_kw = ''.join([f'%{b:02X}' for b in gbk_bytes])

            params = {
                "isnew": 1,
                'kw': "",
                "qw": encoded_kw,
                'rn': 10,  # æ¯é¡µ10æ¡ç»“æœ
                "un": "",
                "only_thread": 0,  # åŒ…å«ä¸»é¢˜å¸–å’Œå›å¤
                "sm": 1,
                "sd": "",
                "ed": ""
            }

            # æ¯é¡µ10æ¡ç»“æœ(rn=10)ï¼Œçˆ¬å–10é¡µ
            for page in range(0, 10):
                params["pn"] = page + 1  # åˆ†é¡µä»1å¼€å§‹
                query = '&'.join([f"{k}={v}" for k, v in params.items()])
                urls.append(f"{base_url}?{query}")

        logger.info(f"Generated {len(urls)} search URLs")
        return urls
    



    #èµ„æºç®¡ç†
    def close(self):
        """èµ„æºæ¸…ç†"""
        self.logger.info("çˆ¬è™«è¿›ç¨‹ç»“æŸ")

#ä¸»ç¨‹åº
if __name__ == '__main__':
    load_dotenv()
    spider = TiebaSpider(kw='æµ™æ±Ÿçœä¸­åŒ»é™¢')
    try:
        # å®Œæ•´æ‰§è¡Œæµç¨‹
        spider.run()  # è°ƒç”¨ä¸»è¿è¡Œæ–¹æ³•
        logger.info("ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
    finally:
        spider.close()  # ç¡®ä¿å…³é—­WebDriver