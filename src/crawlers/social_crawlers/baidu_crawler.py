# src/crawlers/social_crawlers/baidu_crawler.py
import os
import random
import time
import csv
import logging
from urllib.parse import quote
from pathlib import Path
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import requests
import json
from webdriver_manager.chrome import ChromeDriverManager
from requests.adapters import HTTPAdapter
from pymongo import MongoClient, UpdateOne
from dotenv import load_dotenv
from src.utils.api_client import OxylabsScraper  # æ–°å¢å¯¼å…¥
from src.utils.keyword_generator import KeywordGenerator  # æ–°å¢å¯¼å…¥

# é…ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler("logs/tieba_crawler.log", encoding='utf-8'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

class TiebaSpider:
    """ç™¾åº¦è´´å§çˆ¬è™«ï¼ˆå¢å¼ºä»£ç†ç¨³å®šæ€§ç‰ˆï¼‰"""
    #åˆå§‹åŒ–æ–¹æ³•
    def __init__(self, kw='æµ™æ±Ÿçœä¸­åŒ»é™¢', max_page=2, delay=10):
        # åˆå§‹åŒ–å‚æ•°
        self.keyword_tool = KeywordGenerator()  # æ–°å¢
        self.final_keywords = self.keyword_tool.get_encoded_keywords()  # è·å–ç¼–ç åçš„å…³é”®è¯
        self.logger = logging.getLogger(self.__class__.__name__)
        self.keyword = kw
        self.api_client = OxylabsScraper()
        self.base_url = "https://tieba.baidu.com"
        self.data = []
        self.fieldnames = [
            'title',
            'author',
            'content',
            'forum',      # æ–°å¢å­—æ®µ
            'post_time',  # æ–°å¢å­—æ®µ
            'detail_url',
            'reply_count' # ä¿ç•™åŸæœ‰å­—æ®µ
        ]
        
        # MongoDBé…ç½®
        load_dotenv()
        self.client = MongoClient(os.getenv("MONGODB_URI"))
        self.social_db = self.client[os.getenv("SOCIAL_DB", "social_data")]
        self.collection = self.social_db[os.getenv("TIEBA_COLLECTION", "tieba_posts")]

    #æ ¸å¿ƒæ–¹æ³•
    def run(self):
        """ä¸»è¿è¡Œé€»è¾‘ï¼ˆæ•´åˆæœç´¢URLç”Ÿæˆï¼‰"""
        logger.info("ğŸš€ å¯åŠ¨è´´å§æ•°æ®é‡‡é›†å¼•æ“...")

        try:
            search_urls = self.generate_search_urls()  # è°ƒç”¨æ–°å¢çš„URLç”Ÿæˆæ–¹æ³•

            for idx, url in enumerate(search_urls, 1):
                self.data = []  # ğŸ”´ æ–°å¢ï¼šæ¸…ç©ºä¸Šä¸€è½®æ•°æ®
                self.logger.info(f"â–· æ­£åœ¨å¤„ç†ç¬¬ {idx}/{len(search_urls)} ä¸ªæœç´¢æ¡ä»¶ | URL={url[:50]}...")

                # è°ƒç”¨APIè·å–é¡µé¢
                api_response = self.api_client.fetch_page(url)

                if not api_response.get("results"):
                    logger.warning(f"â— ç¬¬ {idx} ä¸ªæœç´¢æ¡ä»¶æ— ç»“æœ")
                    continue

                # è§£æå¹¶å­˜å‚¨æ•°æ®
                self.parse_list_page(api_response["results"][0]["content"])
                self.crawl_details()

                # åŠ¨æ€å»¶è¿Ÿï¼ˆ3-7ç§’ï¼‰
                time.sleep(random.uniform(3, 7))  

            # å­˜å‚¨æœ€ç»ˆæ•°æ®
            if self.data:
                self._save_data()
                logger.info(f"âœ… é‡‡é›†å®Œæˆ | æ€»è®¡è·å– {len(self.data)} æ¡æœ‰æ•ˆæ•°æ®")

        except Exception as e:
            logger.error(f"ğŸ”¥ ä¸»æµç¨‹å¼‚å¸¸ç»ˆæ­¢: {str(e)}", exc_info=True)

        finally:
            self.close()
    
    def _run_static_mode(self):
        """é™æ€è§£ææ¨¡å¼ä¸“ç”¨æµç¨‹"""
        logger.info("ğŸ“„ è¿›å…¥é™æ€è§£ææ¨¡å¼")
        for page in range(1, self.max_page + 1):
            if not self.parse_list_page(page):
                break
            time.sleep(self.delay + random.uniform(1, 3))

        # è·å–è¯¦æƒ…
        logger.info(f"ğŸ” å¼€å§‹å¤„ç† {len(self.data)} æ¡å¸–å­è¯¦æƒ…")
        for idx, item in enumerate(self.data, 1):
            self.get_post_detail(item)
            time.sleep(random.uniform(0.8, 2.2))

    def parse_list_page(self, html: str):
        """è§£æè´´å§åˆ—è¡¨é¡µï¼ˆé€‚é…æ–°ç‰ˆé¡µé¢ç»“æ„ï¼‰"""
        soup = BeautifulSoup(html, "html.parser")
        post_list = soup.select('div.s_post')

        for post in post_list:
            item = {}
            try:
                # ===== æ ‡é¢˜ä¸é“¾æ¥ =====
                title_elem = post.select_one('span.p_title a.bluelink')
                item['title'] = title_elem.text.strip() if title_elem else "æ— æ ‡é¢˜"

                # æ„å»ºå®Œæ•´é“¾æ¥ï¼ˆéœ€å¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
                if title_elem and title_elem.has_attr('href'):
                    item['detail_url'] = f"https://tieba.baidu.com{title_elem['href'].split('?')[0]}"  # å»é™¤å‚æ•°ä¿ç•™çº¯å‡€URL
                else:
                    item['detail_url'] = "é“¾æ¥æ— æ•ˆ"
                
                # ===== æ­£æ–‡å†…å®¹ =====
                content_elem = post.select_one('div.p_content')
                item['content'] = content_elem.text.strip() if content_elem else "å†…å®¹è§£æå¤±è´¥"

                # ===== è´´å§ä¿¡æ¯ =====
                forum_elem = post.select_one('a.p_forum font.p_violet')
                item['forum'] = forum_elem.text.strip() if forum_elem else "æœªçŸ¥è´´å§"

                # ===== ä½œè€…ä¿¡æ¯ =====
                author_elem = post.select_one('a[href^="/home/main"] font.p_violet')  # ç²¾å‡†å®šä½ä½œè€…
                item['author'] = author_elem.text.strip() if author_elem else "åŒ¿åç”¨æˆ·"

                # ===== æ—¶é—´ä¿¡æ¯ =====
                date_elem = post.select_one('font.p_date')
                item['post_time'] = date_elem.text.strip() if date_elem else "æ—¶é—´æœªæ ‡æ³¨"
                
                # è¿½åŠ åˆ°æ•°æ®åˆ—è¡¨
                self.data.append(item)
            except Exception as e:
                self.logger.error(f"è§£æå¼‚å¸¸: {str(e)}")
                # ä¿å­˜é”™è¯¯æ ·æœ¬ç”¨äºè°ƒè¯•
                with open("error_post.html", "w", encoding="utf-8") as f:
                    f.write(str(post))
            
        if not self.data:
            self.logger.warning("âš ï¸ æœªè§£æåˆ°ä»»ä½•å¸–å­æ•°æ®ï¼Œè¯·æ£€æŸ¥HTMLç»“æ„æˆ–é€‰æ‹©å™¨")

    def crawl_details(self):
        """æ‰¹é‡è·å–è¯¦æƒ…é¡µ"""
        for idx, item in enumerate(self.data, 1):
            detail_response = self.api_client.fetch_page(item['detail_url'])
            if detail_html := detail_response.get("results", [{}])[0].get("content"):
                self.parse_detail(item, detail_html)
            self.logger.info(f"è¿›åº¦: {idx}/{len(self.data)}")
            time.sleep(1.5)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡

    def parse_detail(self, item: dict, html: str):
        """è§£æè¯¦æƒ…é¡µ"""
        soup = BeautifulSoup(html, "html.parser")
        content_div = soup.find('div', class_='d_post_content')
        item['content'] = content_div.text.strip() if content_div else ''

    def get_post_detail(self, item):
        """é€šè¿‡ API è·å–è¯¦æƒ…é¡µå†…å®¹"""
        try:
            # è°ƒç”¨ API è·å–è¯¦æƒ…é¡µ
            detail_response = self.api_client.fetch_page(item['detail_url'])

            # æ ¡éªŒ API å“åº”
            if not detail_response.get("results"):
                logger.error(f"API å“åº”å¼‚å¸¸: {detail_response}")
                return
            
            # æå– HTML å†…å®¹
            detail_html = detail_response["results"][0].get("content", "")
            if not detail_html:
                logger.warning(f"è¯¦æƒ…é¡µå†…å®¹ä¸ºç©º: {item['detail_url']}")
                return
            
            # è§£æå†…å®¹
            soup = BeautifulSoup(detail_html, "html.parser")
            content_div = soup.find('div', class_='d_post_content')
            item['content'] = content_div.text.strip() if content_div else ''
            
            
            # éšæœºå»¶è¿Ÿï¼ˆ1-3ç§’ï¼‰
            time.sleep(random.uniform(1, 3))
            
        except Exception as e:
            logger.error(f"è¯¦æƒ…é¡µè·å–å¤±è´¥: {str(e)}")

    #å­˜å‚¨æ–¹æ³•
    def _save_data(self):
        """ç»Ÿä¸€å­˜å‚¨å…¥å£ï¼ˆé›†æˆåŒ»ç–—å†…å®¹è¿‡æ»¤ï¼‰"""
        from src.utils.data_filter import MedicalContentFilter  # å±€éƒ¨å¯¼å…¥é¿å…å¾ªç¯ä¾èµ–

        if not self.data:
            self.logger.warning("âš ï¸ æš‚æ— æ•°æ®å¯å­˜å‚¨")
            return
        
        try:
            # === æ–°å¢è¿‡æ»¤é€»è¾‘ ===
            filter = MedicalContentFilter()
            filtered_data = [item for item in self.data if filter.is_medical_related(item)]

            if not filtered_data:
                self.logger.warning("ğŸ›‘ è¿‡æ»¤åæ— æœ‰æ•ˆåŒ»ç–—æ•°æ®")
                return
            
            # === å­˜å‚¨è¿‡æ»¤åæ•°æ® ===
            mongo_result = self.save_to_mongodb(filtered_data)  # ä¿®æ”¹ä¼ å…¥å‚æ•°
            csv_result = self.save_to_csv(filtered_data)         # ä¿®æ”¹ä¼ å…¥å‚æ•°

            # === æ›´æ–°æ—¥å¿—ä¿¡æ¯ ===
            if mongo_result and csv_result:
                self.logger.info(
                    "ğŸ’¾ å­˜å‚¨æˆåŠŸ | åŸå§‹æ•°æ®: %dæ¡ â†’ æœ‰æ•ˆæ•°æ®: %dæ¡ (è¿‡æ»¤ç‡: %.1f%%)", 
                    len(self.data), 
                    len(filtered_data),
                    (1 - len(filtered_data)/len(self.data)) * 100
                )
            else:
                self.logger.warning("âš ï¸ å­˜å‚¨ç»“æœå¼‚å¸¸ | MongoDB: %s | CSV: %s", mongo_result, csv_result)
                
        except Exception as e:
            self.logger.error("ğŸ’¥ å­˜å‚¨è¿‡ç¨‹å¼‚å¸¸: %s", str(e), exc_info=True)

    def save_to_mongodb(self, data):
        """æ•°æ®å­˜å‚¨ï¼ˆå«å»é‡æœºåˆ¶ï¼‰"""
        if not data:
            logger.warning("âš ï¸ æ— æ•°æ®å¯å­˜å‚¨")
            return False
            
        try:
            # æ•°æ®æ¸…æ´—ä¸åŒ¿ååŒ–
            processed_data = [self._anonymize_data(item.copy()) for item in data]
            
            # æ‰¹é‡å†™å…¥ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
            operations = [
                UpdateOne(
                    {"detail_url": item["detail_url"]},
                    {"$set": item},
                    upsert=True
                ) for item in processed_data
            ]
            result = self.collection.bulk_write(operations, ordered=False)
            logger.info(f"ğŸ“¦ æ•°æ®å†™å…¥å®Œæˆ | æ–°å¢: {result.upserted_count} æ›´æ–°: {result.modified_count} æ€»é‡: {len(data)}æ¡")
            return True
        except Exception as e:
            logger.error(f"æ•°æ®å­˜å‚¨å¤±è´¥: {str(e)}")
            return False

    def save_to_csv(self, data):
        """æœ¬åœ°å¤‡ä»½ï¼ˆä»…ä¿å­˜å¿…è¦å­—æ®µï¼‰"""
        save_dir = Path("data/raw/tieba")
        save_dir.mkdir(parents=True, exist_ok=True)
        file_path = save_dir / f"{self.keyword}_è´´å§æ•°æ®.csv"
        
        try:
            # è‡ªåŠ¨è·å–æ‰€æœ‰å­—æ®µï¼ˆé˜²æ­¢é—æ¼ï¼‰
            all_fields = set()
            for item in data:
                all_fields.update(item.keys())

            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=list(all_fields))
                writer.writeheader()
                writer.writerows(data)

                
            self.logger.info(f"ğŸ’¾ æœ¬åœ°å¤‡ä»½æˆåŠŸ: {file_path}")
            return True
        except Exception as e:
            self.logger.error(f"CSVä¿å­˜å¤±è´¥: {str(e)}")
            return False

    #å·¥å…·æ–¹æ³•
    def _anonymize_data(self, item):
        """æ•°æ®åŒ¿ååŒ–å¤„ç†"""
        # ç§»é™¤ç”¨æˆ·IDå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        item.pop('user_id', None)
        
        # æ—¶é—´æ¨¡ç³Šå¤„ç†ï¼ˆä¿ç•™åˆ°å¤©ï¼‰
        if 'publish_time' in item and isinstance(item['publish_time'], str):
            item['publish_time'] = item['publish_time'].split(' ')[0]
        
        # æ•æ„Ÿè¯æ›¿æ¢
        sensitive_terms = {'è‰¾æ»‹ç—…': 'æŸä¼ æŸ“ç—…', 'ä¹™è‚': 'æŸç—…æ¯’æ€§è‚ç‚'}
        for term, replacement in sensitive_terms.items():
            item['content'] = item['content'].replace(term, replacement)
        return item

    def generate_search_urls(self):
        """ç”Ÿæˆå¤åˆæœç´¢æ¡ä»¶URL"""
        base_url = "https://tieba.baidu.com/f/search/res?ie=utf-8&qw={keyword}"
        return [base_url.format(keyword=kw) for kw in self.final_keywords]  # ç›´æ¥ä½¿ç”¨ç¼–ç åçš„å…³é”®è¯

    #èµ„æºç®¡ç†
    def close(self):
        """èµ„æºæ¸…ç†"""
        self.logger.info("çˆ¬è™«è¿›ç¨‹ç»“æŸ")

#ä¸»ç¨‹åº
if __name__ == '__main__':
    load_dotenv()
    spider = TiebaSpider(kw='æµ™æ±Ÿçœä¸­åŒ»é™¢')
    try:
        # å®Œæ•´æ‰§è¡Œæµç¨‹
        spider.run()  # è°ƒç”¨ä¸»è¿è¡Œæ–¹æ³•
        logger.info("ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
    finally:
        spider.close()  # ç¡®ä¿å…³é—­WebDriver