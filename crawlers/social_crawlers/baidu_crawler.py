# src/crawlers/social_crawlers/baidu_crawler.py
import os
import random
import time
import csv
import logging
from urllib.parse import quote
from pathlib import Path
from fake_useragent import UserAgent
from bs4 import BeautifulSoup
import requests
import json
from webdriver_manager.chrome import ChromeDriverManager
from requests.adapters import HTTPAdapter
from pymongo import MongoClient, UpdateOne
from dotenv import load_dotenv
from src.utils.api_client import OxylabsScraper  # æ–°å¢å¯¼å…¥

# é…ç½®æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler("logs/tieba_crawler.log", encoding='utf-8'), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

class TiebaSpider:
    """ç™¾åº¦è´´å§çˆ¬è™«ï¼ˆå¢å¼ºä»£ç†ç¨³å®šæ€§ç‰ˆï¼‰"""
    #åˆå§‹åŒ–æ–¹æ³•
    def __init__(self, kw='æµ™æ±Ÿçœä¸­åŒ»é™¢', max_page=2, delay=10):
        # åˆå§‹åŒ–å‚æ•°
        self.logger = logging.getLogger(self.__class__.__name__)
        self.keyword = kw
        self.api_client = OxylabsScraper()
        self.base_url = "https://tieba.baidu.com"
        self.data = []
        self.fieldnames = [
            'title',
            'author',
            'content',
            'forum',      # æ–°å¢å­—æ®µ
            'post_time',  # æ–°å¢å­—æ®µ
            'detail_url',
            'reply_count' # ä¿ç•™åŸæœ‰å­—æ®µ
        ]
        
        # MongoDBé…ç½®
        load_dotenv()
        self.client = MongoClient(os.getenv("MONGODB_URI"))
        self.social_db = self.client[os.getenv("SOCIAL_DB", "social_data")]
        self.collection = self.social_db[os.getenv("TIEBA_COLLECTION", "tieba_posts")]

    #æ ¸å¿ƒæ–¹æ³•
    def run(self):
        """ä¸»è¿è¡Œé€»è¾‘ï¼ˆçº¯APIæ¨¡å¼ï¼‰"""
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–ã€{self.keyword}å§ã€‘...")
        
        # é€šè¿‡ API è·å–åˆ—è¡¨é¡µ
        list_url = f"{self.base_url}/f/search/res?ie=utf-8&qw={self.keyword}"
        api_response = self.api_client.fetch_page(list_url)

        # æ–°å¢å“åº”éªŒè¯
        if not isinstance(api_response, dict) or "results" not in api_response:
            logger.error(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {api_response}")
            return
        if not api_response["results"]:
            logger.error("åˆ—è¡¨é¡µè·å–å¤±è´¥")
            return
        self.logger.debug(f"APIå“åº”æ‘˜è¦: {str(api_response)[:200]}")  # æ‰“å°å‰200å­—ç¬¦

        if not api_response.get("results"):
            self.logger.error("åˆ—è¡¨é¡µè·å–å¤±è´¥")
            return
        
        # è§£æå¸–å­åˆ—è¡¨
        html_content = api_response["results"][0]["content"]

        # æ–°å¢ï¼šä¿å­˜åŸå§‹HTMLç”¨äºåˆ†æ
        with open("raw_html.html", "w", encoding="utf-8") as f:
            f.write(html_content)
        self.parse_list_page(html_content)

        # è·å–å¸–å­è¯¦æƒ…
        self.crawl_details()

        # å­˜å‚¨æ•°æ®
        if self.data:
            self._save_data()
    
    def _run_static_mode(self):
        """é™æ€è§£ææ¨¡å¼ä¸“ç”¨æµç¨‹"""
        logger.info("ğŸ“„ è¿›å…¥é™æ€è§£ææ¨¡å¼")
        for page in range(1, self.max_page + 1):
            if not self.parse_list_page(page):
                break
            time.sleep(self.delay + random.uniform(1, 3))

        # è·å–è¯¦æƒ…
        logger.info(f"ğŸ” å¼€å§‹å¤„ç† {len(self.data)} æ¡å¸–å­è¯¦æƒ…")
        for idx, item in enumerate(self.data, 1):
            self.get_post_detail(item)
            time.sleep(random.uniform(0.8, 2.2))

    def parse_list_page(self, html: str):
        """è§£æè´´å§åˆ—è¡¨é¡µï¼ˆé€‚é…æ–°ç‰ˆé¡µé¢ç»“æ„ï¼‰"""
        soup = BeautifulSoup(html, "html.parser")
        post_list = soup.select('div.s_post')

        for post in post_list:
            item = {}
            try:
                # ===== æ ‡é¢˜ä¸é“¾æ¥ =====
                title_elem = post.select_one('span.p_title a.bluelink')
                item['title'] = title_elem.text.strip() if title_elem else "æ— æ ‡é¢˜"

                # æ„å»ºå®Œæ•´é“¾æ¥ï¼ˆéœ€å¤„ç†ç›¸å¯¹è·¯å¾„ï¼‰
                if title_elem and title_elem.has_attr('href'):
                    item['detail_url'] = f"https://tieba.baidu.com{title_elem['href'].split('?')[0]}"  # å»é™¤å‚æ•°ä¿ç•™çº¯å‡€URL
                else:
                    item['detail_url'] = "é“¾æ¥æ— æ•ˆ"
                
                # ===== æ­£æ–‡å†…å®¹ =====
                content_elem = post.select_one('div.p_content')
                item['content'] = content_elem.text.strip() if content_elem else "å†…å®¹è§£æå¤±è´¥"

                # ===== è´´å§ä¿¡æ¯ =====
                forum_elem = post.select_one('a.p_forum font.p_violet')
                item['forum'] = forum_elem.text.strip() if forum_elem else "æœªçŸ¥è´´å§"

                # ===== ä½œè€…ä¿¡æ¯ =====
                author_elem = post.select_one('a[href^="/home/main"] font.p_violet')  # ç²¾å‡†å®šä½ä½œè€…
                item['author'] = author_elem.text.strip() if author_elem else "åŒ¿åç”¨æˆ·"

                # ===== æ—¶é—´ä¿¡æ¯ =====
                date_elem = post.select_one('font.p_date')
                item['post_time'] = date_elem.text.strip() if date_elem else "æ—¶é—´æœªæ ‡æ³¨"
                
                # è¿½åŠ åˆ°æ•°æ®åˆ—è¡¨
                self.data.append(item)
            except Exception as e:
                self.logger.error(f"è§£æå¼‚å¸¸: {str(e)}")
                # ä¿å­˜é”™è¯¯æ ·æœ¬ç”¨äºè°ƒè¯•
                with open("error_post.html", "w", encoding="utf-8") as f:
                    f.write(str(post))
            
        if not self.data:
            self.logger.warning("âš ï¸ æœªè§£æåˆ°ä»»ä½•å¸–å­æ•°æ®ï¼Œè¯·æ£€æŸ¥HTMLç»“æ„æˆ–é€‰æ‹©å™¨")

    def crawl_details(self):
        """æ‰¹é‡è·å–è¯¦æƒ…é¡µ"""
        for idx, item in enumerate(self.data, 1):
            detail_response = self.api_client.fetch_page(item['detail_url'])
            if detail_html := detail_response.get("results", [{}])[0].get("content"):
                self.parse_detail(item, detail_html)
            self.logger.info(f"è¿›åº¦: {idx}/{len(self.data)}")
            time.sleep(1.5)  # æ§åˆ¶è¯·æ±‚é¢‘ç‡

    def parse_detail(self, item: dict, html: str):
        """è§£æè¯¦æƒ…é¡µ"""
        soup = BeautifulSoup(html, "html.parser")
        content_div = soup.find('div', class_='d_post_content')
        item['content'] = content_div.text.strip() if content_div else ''

    def get_post_detail(self, item):
        """é€šè¿‡ API è·å–è¯¦æƒ…é¡µå†…å®¹"""
        try:
            # è°ƒç”¨ API è·å–è¯¦æƒ…é¡µ
            detail_response = self.api_client.fetch_page(item['detail_url'])

            # æ ¡éªŒ API å“åº”
            if not detail_response.get("results"):
                logger.error(f"API å“åº”å¼‚å¸¸: {detail_response}")
                return
            
            # æå– HTML å†…å®¹
            detail_html = detail_response["results"][0].get("content", "")
            if not detail_html:
                logger.warning(f"è¯¦æƒ…é¡µå†…å®¹ä¸ºç©º: {item['detail_url']}")
                return
            
            # è§£æå†…å®¹
            soup = BeautifulSoup(detail_html, "html.parser")
            content_div = soup.find('div', class_='d_post_content')
            item['content'] = content_div.text.strip() if content_div else ''
            
            
            # éšæœºå»¶è¿Ÿï¼ˆ1-3ç§’ï¼‰
            time.sleep(random.uniform(1, 3))
            
        except Exception as e:
            logger.error(f"è¯¦æƒ…é¡µè·å–å¤±è´¥: {str(e)}")

    #å­˜å‚¨æ–¹æ³•
    def _save_data(self):
        """ç»Ÿä¸€å­˜å‚¨å…¥å£"""
        if not self.data:
            self.logger.warning("âš ï¸ æš‚æ— æ•°æ®å¯å­˜å‚¨")
            return
        try:
            # MongoDBå­˜å‚¨
            mongo_result = self.save_to_mongodb()
            # CSVå¤‡ä»½
            csv_result = self.save_to_csv()

            if mongo_result and csv_result:
                logger.info("ğŸ’¾ å­˜å‚¨æˆåŠŸ | MongoDB: %dæ¡ | CSV: %dæ¡", 
                            len(self.data), len(self.data))
            else:
                logger.warning("âš ï¸ å­˜å‚¨ç»“æœå¼‚å¸¸ | MongoDB: %s | CSV: %s", 
                               mongo_result, csv_result)
        except Exception as e:
            logger.error("ğŸ’¥ å­˜å‚¨è¿‡ç¨‹å¼‚å¸¸: %s", str(e), exc_info=True)

    def save_to_mongodb(self):
        """æ•°æ®å­˜å‚¨ï¼ˆå«å»é‡æœºåˆ¶ï¼‰"""
        if not self.data:
            return
            
        try:
            # æ•°æ®æ¸…æ´—ä¸åŒ¿ååŒ–
            processed_data = [self._anonymize_data(item.copy()) for item in self.data]
            
            # æ‰¹é‡å†™å…¥ï¼ˆè‡ªåŠ¨å»é‡ï¼‰
            operations = [
                UpdateOne(
                    {"detail_url": item["detail_url"]},
                    {"$set": item},
                    upsert=True
                ) for item in processed_data
            ]
            result = self.collection.bulk_write(operations, ordered=False)
            logger.info(f"ğŸ“¦ æ•°æ®å†™å…¥å®Œæˆ | æ–°å¢: {result.upserted_count} æ›´æ–°: {result.modified_count}")
        except Exception as e:
            logger.error(f"æ•°æ®å­˜å‚¨å¤±è´¥: {str(e)}")

    def save_to_csv(self):
        """æœ¬åœ°å¤‡ä»½ï¼ˆä»…ä¿å­˜å¿…è¦å­—æ®µï¼‰"""
        save_dir = Path("data/raw/tieba")
        save_dir.mkdir(parents=True, exist_ok=True)
        file_path = save_dir / f"{self.keyword}_è´´å§æ•°æ®.csv"
        
        try:
            # è‡ªåŠ¨è·å–æ‰€æœ‰å­—æ®µï¼ˆé˜²æ­¢é—æ¼ï¼‰
            all_fields = set()
            for item in self.data:
                all_fields.update(item.keys())

            # å†™å…¥æ–‡ä»¶
            with open(file_path, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=list(all_fields))
                writer.writeheader()
                writer.writerows(self.data)

                
            self.logger.info(f"ğŸ’¾ æœ¬åœ°å¤‡ä»½æˆåŠŸ: {file_path}")
            return True
        except Exception as e:
            self.logger.error(f"CSVä¿å­˜å¤±è´¥: {str(e)}")
            return False

    #å·¥å…·æ–¹æ³•
    def _anonymize_data(self, item):
        """æ•°æ®åŒ¿ååŒ–å¤„ç†"""
        # ç§»é™¤ç”¨æˆ·IDå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        item.pop('user_id', None)
        
        # æ—¶é—´æ¨¡ç³Šå¤„ç†ï¼ˆä¿ç•™åˆ°å¤©ï¼‰
        if 'publish_time' in item and isinstance(item['publish_time'], str):
            item['publish_time'] = item['publish_time'].split(' ')[0]
        
        # æ•æ„Ÿè¯æ›¿æ¢
        sensitive_terms = {'è‰¾æ»‹ç—…': 'æŸä¼ æŸ“ç—…', 'ä¹™è‚': 'æŸç—…æ¯’æ€§è‚ç‚'}
        for term, replacement in sensitive_terms.items():
            item['content'] = item['content'].replace(term, replacement)
        return item

    #èµ„æºç®¡ç†
    def close(self):
        """èµ„æºæ¸…ç†"""
        self.logger.info("çˆ¬è™«è¿›ç¨‹ç»“æŸ")

#ä¸»ç¨‹åº
if __name__ == '__main__':
    load_dotenv()
    spider = TiebaSpider(kw='æµ™æ±Ÿçœä¸­åŒ»é™¢')
    try:
        # å®Œæ•´æ‰§è¡Œæµç¨‹
        spider.run()  # è°ƒç”¨ä¸»è¿è¡Œæ–¹æ³•
        logger.info("ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
    finally:
        spider.close()  # ç¡®ä¿å…³é—­WebDriver