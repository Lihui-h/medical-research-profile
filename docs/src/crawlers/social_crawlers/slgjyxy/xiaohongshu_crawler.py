# src/crawlers/social_crawlers/slgjyxy/xiaohongshu_crawler.py
import os
import re
import json
import time
import random
import logging
from urllib.parse import urljoin, urlparse, parse_qs
from datetime import datetime, timedelta
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from src.utils.api_client import OxylabsScraper
from src.crawlers.social_crawlers.slgjyxy.keyword_generator import KeywordGenerator
from supabase import create_client, Client

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs/xiaohongshu_crawler.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class XiaohongshuSpider:
    """å°çº¢ä¹¦ä¸¤é˜¶æ®µçˆ¬è™«ï¼ˆ2025ä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, kw='æµ™æ±Ÿæ ‘äºº'):
        # åˆå§‹åŒ–å‚æ•°
        self.keyword_tool = KeywordGenerator()
        self.keyword = kw
        self.api_client = OxylabsScraper()
        self.base_url = "https://www.xiaohongshu.com"
        self.search_url = "https://www.xiaohongshu.com/search_result?keyword=%25E6%25B5%2599%25E6%25B1%259F%25E6%25A0%2591%25E4%25BA%25BA&source=web_explore_feed"
        self.data = []
        self.detail_urls = []  # å­˜å‚¨éœ€è¦çˆ¬å–çš„è¯¦æƒ…é¡µURL
        
        # åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨
        self.sentiment_analyzer = WeightedSentimentAnalyzer()
        
        load_dotenv()
        self.supabase: Client = create_client(
            os.getenv("SUPABASE_URL"),
            os.getenv("SUPABASE_KEY") 
        )
        
        # æ ‘å…°å›½é™…åŒ»å­¦é™¢ä¸“ç”¨é…ç½®
        self.org_code = "slgjyxy"
        self.institution_name = "æ ‘å…°å›½é™…åŒ»å­¦é™¢"
        self.user_id = os.getenv("SUPABASE_USER_UUID_SLGJYXY")

    def run(self):
        """ä¸»è¿è¡Œé€»è¾‘"""
        logger.info("å¯åŠ¨å°çº¢ä¹¦æ•°æ®é‡‡é›†å¼•æ“...")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šè·å–æœç´¢ç»“æœé¡µä¸­çš„è¯¦æƒ…é“¾æ¥
        logger.info(f"çˆ¬å–å›ºå®šæœç´¢é¡µ: {self.search_url}")
            
            # è°ƒç”¨APIè·å–æœç´¢é¡µæ•°æ®
        api_response = self.api_client.fetch_page(self.search_url)
        if not api_response.get("results"):
            logger.warning("APIå“åº”ä¸ºç©ºï¼Œè·³è¿‡")
            return
                
        # è§£ææœç´¢é¡µï¼Œæå–è¯¦æƒ…é¡µURL
        self.parse_search_page(api_response["results"][0]["content"])
            
        # åˆç†å»¶è¿Ÿ
        time.sleep(random.uniform(2.0, 4.0))
        
        logger.info(f"ä»æœç´¢é¡µæå–åˆ° {len(self.detail_urls)} ä¸ªè¯¦æƒ…é¡µé“¾æ¥")
        
        # ç¬¬äºŒé˜¶æ®µï¼šçˆ¬å–è¯¦æƒ…é¡µå†…å®¹
        total_details = len(self.detail_urls)
        for idx, url in enumerate(self.detail_urls, 1):
            logger.info(f"çˆ¬å–è¯¦æƒ…é¡µ [{idx}/{total_details}]: {url[:80]}...")
            
            # è°ƒç”¨APIè·å–è¯¦æƒ…é¡µæ•°æ®
            detail_response = self.api_client.fetch_page(url)
            if not detail_response.get("results"):
                logger.warning("è¯¦æƒ…é¡µAPIå“åº”ä¸ºç©ºï¼Œè·³è¿‡")
                continue
                
            # è§£æè¯¦æƒ…é¡µå†…å®¹
            self.parse_detail_page(detail_response["results"][0]["content"], url)
            
            # æ›´é•¿çš„éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹Ÿäººå·¥æ“ä½œ
            delay = random.uniform(3.0, 8.0)
            logger.debug(f"éšæœºå»¶è¿Ÿ {delay:.1f} ç§’")
            time.sleep(delay)
            
            # æ¯å¤„ç†5ä¸ªè¯¦æƒ…é¡µä¿å­˜ä¸€æ¬¡æ•°æ®
            if idx % 5 == 0:
                self._save_data()
        
        # ä¿å­˜å‰©ä½™æ•°æ®
        if self.data:
            self._save_data()
        
        logger.info(f"å…¨éƒ¨é‡‡é›†å®Œæˆ | æ€»æ•°æ®é‡: {len(self.data)} æ¡")

    def parse_search_page(self, html: str):
        """è§£ææœç´¢ç»“æœé¡µï¼Œæå–è¯¦æƒ…é¡µé“¾æ¥"""
        soup = BeautifulSoup(html, "html.parser")
        
        # æ–¹æ³•1ï¼šç›´æ¥å®šä½é“¾æ¥å…ƒç´ ï¼ˆæœ€å¯é ï¼‰
        links = soup.find_all('a', href=re.compile(r'/search_result/'))

        if links:
            logger.info(f"ç›´æ¥æ‰¾åˆ° {len(links)} ä¸ªé“¾æ¥")
            for link in links:
                try:
                    if link.has_attr('href'):
                        href = link['href']
                        if not href.startswith("http"):
                            href = urljoin(self.base_url, href)
                        self.detail_urls.append(href)
                        logger.info(f"æ·»åŠ è¯¦æƒ…é¡µ: {href}")
                    else:
                        logger.warning("é“¾æ¥ç¼ºå°‘hrefå±æ€§")
                except Exception as e:
                    logger.error(f"å¤„ç†é“¾æ¥å¼‚å¸¸: {str(e)}")
            return

        # æ–¹æ³•2ï¼šä½¿ç”¨æ›´å¹¿æ³›çš„æœç´¢
        logger.warning("ç›´æ¥å®šä½é“¾æ¥å¤±è´¥ï¼Œå°è¯•æ›´å¹¿æ³›çš„æœç´¢...")
        all_links = soup.find_all('a')
        found_links = 0

        for link in all_links:
            try:
                if link.has_attr('href') and "/search_result/" in link['href']:
                    href = link['href']
                    if not href.startswith("http"):
                        href = urljoin(self.base_url, href)
                    self.detail_urls.append(href)
                    found_links += 1
                    logger.info(f"æ·»åŠ è¯¦æƒ…é¡µ: {href}")
            except Exception as e:
                logger.error(f"å¤„ç†é“¾æ¥å¼‚å¸¸: {str(e)}")

        if found_links:
            logger.info(f"é€šè¿‡å¹¿æ³›æœç´¢æ‰¾åˆ° {found_links} ä¸ªé“¾æ¥")
            return
        
        # æ–¹æ³•3ï¼šæœ€åå°è¯•
        logger.warning("æ‰€æœ‰é“¾æ¥æŸ¥æ‰¾æ–¹æ³•å¤±è´¥ï¼Œå°è¯•æå–data-v-*å…ƒç´ ")
        note_items = []

        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«data-v-å±æ€§çš„å…ƒç´ 
        all_elements = soup.find_all(True)
        for element in all_elements:
            if element.name in ['section', 'div']:
                for attr_name in element.attrs:
                    if attr_name.startswith('data-v-'):
                        note_items.append(element)
                        break
            
        if not note_items:
            logger.warning("æœªæ‰¾åˆ°ç¬”è®°å¡ç‰‡ï¼Œå°è¯•å¤‡ç”¨é€‰æ‹©å™¨...")
            # å°è¯•ç±»åé€‰æ‹©å™¨
            note_items = soup.select('section.note-item, div.note-item, div.feed-item')
        
        if not note_items:
            logger.warning("æ‰€æœ‰é€‰æ‹©å™¨å‡å¤±è´¥ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜æ›´")
            return
        
        logger.info(f"æ‰¾åˆ° {len(note_items)} ä¸ªç¬”è®°å¡ç‰‡")
        
        for item in note_items:
            try:
                # å°è¯•åœ¨å®¹å™¨å†…æŸ¥æ‰¾é“¾æ¥
                link_element = item.find('a', href=lambda x: x and "/search_result/" in x)
                
                if link_element and link_element.has_attr('href'):
                    href = link_element['href']
                    
                    # æ¸…ç†URLå‚æ•°ï¼Œä¿ç•™å…³é”®éƒ¨åˆ†
                    parsed = urlparse(href)
                    path = parsed.path
                    
                    # æå–å…³é”®å‚æ•°ï¼ˆå¦‚note_idï¼‰
                    note_id = path.split('/')[-1] if path else None
                    query_params = parse_qs(parsed.query)
                    
                    # æ„å»ºæ ‡å‡†åŒ–çš„è¯¦æƒ…é¡µURL
                    if note_id:
                        # ä¿ç•™å¿…è¦çš„å‚æ•°
                        essential_params = {
                            "source": "web_explore_feed",
                            "xsec_source": "pc_search"
                        }
                        
                        # å¦‚æœæœ‰tokenåˆ™ä¿ç•™
                        if 'xsec_token' in query_params:
                            essential_params['xsec_token'] = query_params['xsec_token'][0]
                        
                        # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²
                        query_str = "&".join([f"{k}={v}" for k, v in essential_params.items()])
                        
                        # æ„å»ºå®Œæ•´URL
                        detail_url = f"{self.base_url}/explore/{note_id}?{query_str}"
                        self.detail_urls.append(detail_url)
                        logger.debug(f"æ·»åŠ è¯¦æƒ…é¡µ: {detail_url}")
                    else:
                        logger.warning(f"æ— æ³•ä»é“¾æ¥æå–note_id: {href}")
                else:
                    logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆé“¾æ¥å…ƒç´ ")
                    
            except Exception as e:
                logger.error(f"è§£ææœç´¢é¡µå¼‚å¸¸: {str(e)}")

    def parse_detail_page(self, html: str, url: str):
        """è§£æè¯¦æƒ…é¡µå†…å®¹"""
        soup = BeautifulSoup(html, "html.parser")
        item = {"detail_url": url}
        
        try:
            # ==== æ ‡é¢˜ ====
            title_elem = soup.select_one('h1.title, div.title, h2.title')
            item['title'] = title_elem.text.strip() if title_elem else "æ— æ ‡é¢˜"
            
            # ==== å†…å®¹ ====
            content_elem = soup.select_one('div.content, div.note-content, span.note-text')
            item['content'] = content_elem.text.strip() if content_elem else ""
            
            # ==== ä½œè€… ====
            author_elem = soup.select_one('a.name, div.author-info, span.username')
            item['author'] = author_elem.text.strip() if author_elem else "åŒ¿åç”¨æˆ·"
            
            # ==== å‘å¸ƒæ—¶é—´ ====
            time_elem = soup.select_one('time.date, span.data, div.date')
            if time_elem:
                # è½¬æ¢ç›¸å¯¹æ—¶é—´ï¼ˆå¦‚"3å¤©å‰"ï¼‰
                raw_time = time_elem.text.strip()
                item['raw_post_time'] = self.parse_relative_time(raw_time)
            else:
                item['raw_post_time'] = datetime.now().strftime("%Y-%m-%d")
            
            # ==== äº’åŠ¨æ•°æ® ==== (ä½¿ç”¨æ›´ç²¾ç¡®çš„æ–¹æ³•)
            interactions = self.extract_interaction_data(soup)
            item.update(interactions)
            
            # ==== æœºæ„ç›¸å…³å…ƒæ•°æ® ====
            item.update({
                'source': 'xiaohongshu',
                'institution_name': self.institution_name,
                'org_code': self.org_code,
                'user_id': self.user_id
            })
            
            # æƒ…æ„Ÿåˆ†æ
            full_text = f"{item['title']} {item['content']}"
            sentiment_result = self.sentiment_analyzer.analyze(full_text)
            item['sentiment'] = sentiment_result[0]
            item['sentiment_score'] = sentiment_result[1]
            
            # æœ‰æ•ˆæ€§æ ¡éªŒ
            required_keywords = ['æ ‘äºº', 'æ ‘å…°', 'æ ‘å¤§', 'æ ‘é™¢']
            has_keyword = any(kw in full_text for kw in required_keywords)
            
            valid_check = [
                len(item.get('content', '')) > 20,
                has_keyword
            ]
            
            if not all(valid_check):
                reasons = []
                if not valid_check[0]: reasons.append(f"å†…å®¹è¿‡çŸ­({len(item.get('content',''))}å­—)")
                if not valid_check[1]: reasons.append("æœªå«å…³é”®è¯")
                logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®: {', '.join(reasons)}")
                return
            
            self.data.append(item)
            logger.info(f"æˆåŠŸè§£æè¯¦æƒ…é¡µ: {item['title'][:20]}...")
            
        except Exception as e:
            logger.error(f"è§£æè¯¦æƒ…é¡µå¼‚å¸¸: {str(e)}")
            # ä¿å­˜é”™è¯¯é¡µç”¨äºè°ƒè¯•
            timestamp = int(time.time())
            with open(f"error_detail_{timestamp}.html", "w", encoding="utf-8") as f:
                f.write(html)

    def extract_interaction_data(self, soup):
        """ç²¾ç¡®æå–äº’åŠ¨æ•°æ®ï¼ˆç‚¹èµã€æ”¶è—ã€è¯„è®ºï¼‰"""
        interactions = {}
        
        # ç‚¹èµæ•° - ä½¿ç”¨ç²¾ç¡®é€‰æ‹©å™¨
        likes = soup.select_one('span.like-wrapper span.count, span.like-count')
        interactions['likes'] = self.convert_count(likes.text.strip()) if likes else 0
        
        # æ”¶è—æ•° - ä½¿ç”¨ç²¾ç¡®é€‰æ‹©å™¨
        collects = soup.select_one('span.collect-wrapper span.count, span.collect-count')
        interactions['collects'] = self.convert_count(collects.text.strip()) if collects else 0
        
        # è¯„è®ºæ•° - ä½¿ç”¨ç²¾ç¡®é€‰æ‹©å™¨
        comments = soup.select_one('span.chat-wrapper span.count, span.comment-count')
        interactions['comments'] = self.convert_count(comments.text.strip()) if comments else 0
        
        return interactions

    def parse_relative_time(self, time_str):
        """è½¬æ¢ç›¸å¯¹æ—¶é—´ä¸ºæ ‡å‡†æ—¥æœŸæ ¼å¼"""
        now = datetime.now()
        
        if "åˆ†é’Ÿ" in time_str:
            mins = int(re.search(r'\d+', time_str).group())
            return (now - timedelta(minutes=mins)).strftime("%Y-%m-%d")
        elif "å°æ—¶" in time_str:
            hours = int(re.search(r'\d+', time_str).group())
            return (now - timedelta(hours=hours)).strftime("%Y-%m-%d")
        elif "å¤©" in time_str:
            days = int(re.search(r'\d+', time_str).group())
            return (now - timedelta(days=days)).strftime("%Y-%m-%d")
        elif "å‘¨" in time_str:
            weeks = int(re.search(r'\d+', time_str).group())
            return (now - timedelta(weeks=weeks)).strftime("%Y-%m-%d")
        elif "æœˆ" in time_str:
            months = int(re.search(r'\d+', time_str).group())
            return (now - timedelta(days=months*30)).strftime("%Y-%m-%d")
        elif "å¹´" in time_str:
            years = int(re.search(r'\d+', time_str).group())
            return (now - timedelta(days=years*365)).strftime("%Y-%m-%d")
        else:
            # å°è¯•è§£ææ ‡å‡†æ—¥æœŸæ ¼å¼
            try:
                datetime.strptime(time_str, "%Y-%m-%d")
                return time_str
            except:
                return datetime.now().strftime("%Y-%m-%d")

    def convert_count(self, text):
        """è½¬æ¢äº’åŠ¨è®¡æ•°æ–‡æœ¬ä¸ºæ•°å­—"""
        if 'ä¸‡' in text:
            num = float(re.search(r'[\d.]+', text).group())
            return int(num * 10000)
        elif 'åƒ' in text:
            num = float(re.search(r'[\d.]+', text).group())
            return int(num * 1000)
        else:
            return int(re.search(r'\d+', text).group()) if re.search(r'\d+', text) else 0

    def _save_data(self):
        """å­˜å‚¨æ•°æ®åˆ°Supabase"""
        if not self.data:
            logger.warning("æ— æœ‰æ•ˆæ•°æ®å¯å­˜å‚¨")
            return
        
        try:
            # æ‰¹é‡æ’å…¥å¹¶å»é‡
            response = self.supabase.table('posts').upsert(
                self.data,
                on_conflict='detail_url'
            ).execute()
            
            if response.data and len(response.data) > 0:
                logger.info(f"æˆåŠŸå†™å…¥ {len(response.data)} æ¡æ•°æ®")
                self.data = []  # æ¸…ç©ºå·²ä¿å­˜æ•°æ®
            else:
                logger.warning("æ— æ–°æ•°æ®å†™å…¥")
        except Exception as e:
            logger.error(f"å­˜å‚¨å¤±è´¥: {str(e)}")

class WeightedSentimentAnalyzer:
    """å°çº¢ä¹¦ä¸“ç”¨æƒ…æ„Ÿåˆ†æå™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        # è´Ÿé¢è¯æƒé‡ï¼ˆé’ˆå¯¹æ•™è‚²é¢†åŸŸä¼˜åŒ–ï¼‰
        self.negative_weights = {
            "å·®è¯„": -4, "å¤±æœ›": -3, "é¿é›·": -4, "åæ§½": -3, 
            "å‘": -3, "ä¸æ¨è": -3, "åæ‚”": -3, "å·®åŠ²": -4,
            "ä¸è¡Œ": -2, "ä¸å¥½": -2, "åƒåœ¾": -5, "æŠ•è¯‰": -4
        }
        
        # æ­£é¢è¯æƒé‡
        self.positive_weights = {
            "æ¨è": +3, "æ»¡æ„": +3, "ç‚¹èµ": +3, "ä¼˜ç§€": +4,
            "ä¸“ä¸š": +4, "è¶…èµ": +4, "å€¼å¾—": +3, "å–œæ¬¢": +2,
            "å¥½": +1, "æ£’": +2, "å¼º": +2, "å‰å®³": +3
        }
        
        # æ•™è‚²é¢†åŸŸç‰¹å®šè¯
        self.education_weights = {
            "å¸ˆèµ„": +3, "æ•™å­¦": +3, "å­¦ä¹ æ°›å›´": +4, "å®è·µæœºä¼š": +3,
            "å°±ä¸š": +4, "è€ƒç ”": +3, "å¸ˆèµ„åŠ›é‡": +4, "æ ¡å›­ç¯å¢ƒ": +2,
            "è¯¾ç¨‹": +2, "ä¸“ä¸šè®¾ç½®": +3, "å­¦è´¹": -2, "å®¿èˆæ¡ä»¶": +1
        }
        
        self.thresholds = (-3, 3)  # (negative_threshold, positive_threshold)

    def calculate_score(self, text):
        """åŠ æƒè¯„åˆ†ç®—æ³•ï¼ˆæ•™è‚²é¢†åŸŸä¼˜åŒ–ç‰ˆï¼‰"""
        score = 0
        
        # è´Ÿé¢è¯æ£€æµ‹
        for word, weight in self.negative_weights.items():
            if word in text:
                score += weight * text.count(word)
                
        # æ­£é¢è¯æ£€æµ‹
        for word, weight in self.positive_weights.items():
            if word in text:
                score += weight * text.count(word)
                
        # æ•™è‚²é¢†åŸŸå…³é”®è¯æ£€æµ‹
        for word, weight in self.education_weights.items():
            if word in text:
                score += weight * text.count(word)
                
        # è¡¨æƒ…ç¬¦å·å¢å¼º
        if "ğŸ‘" in text or "â¤ï¸" in text:
            score += 2
        if "ğŸ‘" in text or "ğŸ’”" in text:
            score -= 2
            
        return score

    def analyze(self, text):
        """æƒ…æ„Ÿåˆ†ç±»"""
        score = self.calculate_score(text)
        if score <= self.thresholds[0]:
            return ("negative", score)
        elif score >= self.thresholds[1]:
            return ("positive", score)
        else:
            return ("neutral", score)

if __name__ == '__main__':
    load_dotenv()
    
    # è¿è¡Œçˆ¬è™«
    spider = XiaohongshuSpider(kw='æµ™æ±Ÿæ ‘äºº')
    try:
        spider.run()
    except Exception as e:
        logger.error(f"çˆ¬è™«å¼‚å¸¸ç»ˆæ­¢: {str(e)}")