import os
import re
import json
import time
import logging
from urllib.parse import quote
from datetime import datetime
from dotenv import load_dotenv
from bs4 import BeautifulSoup
from src.utils.api_client import OxylabsScraper
from src.crawlers.social_crawlers.slgjyxy.keyword_generator import KeywordGenerator
from supabase import create_client, Client

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.FileHandler("logs/xiaohongshu_crawler.log"), logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

class XiaohongshuSpider:
    """å°çº¢ä¹¦çˆ¬è™«ï¼ˆå¢å¼ºä»£ç†ç¨³å®šæ€§ç‰ˆï¼‰"""
    
    def __init__(self, kw='æµ™æ±Ÿæ ‘äºº'):
        # åˆå§‹åŒ–å‚æ•°
        self.keyword_tool = KeywordGenerator()
        self.final_keywords = self.keyword_tool.get_encoded_keywords()
        self.keyword = kw
        self.api_client = OxylabsScraper()
        self.base_url = "https://www.xiaohongshu.com"
        self.data = []
        
        # åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨
        self.sentiment_analyzer = WeightedSentimentAnalyzer()
        
        load_dotenv()
        self.supabase: Client = create_client(
            os.getenv("SUPABASE_URL"),
            os.getenv("SUPABASE_KEY") 
        )
        
        # æ ‘å…°å›½é™…åŒ»å­¦é™¢ä¸“ç”¨é…ç½®
        self.org_code = "slgjyxy"
        self.institution_name = "æ ‘å…°å›½é™…åŒ»å­¦é™¢"
        self.user_id = os.getenv("SUPABASE_USER_UUID_SLGJYXY")

    def run(self):
        """ä¸»è¿è¡Œé€»è¾‘"""
        logger.info("ğŸš€ å¯åŠ¨å°çº¢ä¹¦æ•°æ®é‡‡é›†å¼•æ“...")
        search_urls = self.generate_search_urls()
        
        total_count = 0
        for idx, url in enumerate(search_urls, 1):
            logger.info(f"â–· å¤„ç†URL [{idx}/{len(search_urls)}]: {url[:80]}...")
            
            # è°ƒç”¨APIè·å–æ•°æ®
            api_response = self.api_client.fetch_page(url)
            if not api_response.get("results"):
                continue
                
            # è§£æåˆ—è¡¨é¡µ
            self.parse_list_page(api_response["results"][0]["content"])
            if not self.data:
                logger.warning("âš ï¸ æœªè·å–åˆ°ä»»ä½•ç¬”è®°æ•°æ®")
                continue
            
            # ç«‹å³å­˜å‚¨å½“å‰æ‰¹æ¬¡æ•°æ®
            self._save_data()
            total_count += len(self.data)
            logger.info(f"âœ… å·²è·å– {len(self.data)} æ¡ç¬”è®°æ•°æ® | ç´¯è®¡: {total_count} æ¡")
            self.data.clear()
            
            # åˆç†å»¶è¿Ÿ
            time.sleep(random.uniform(2.0, 4.0))
        
        logger.info(f"ğŸ å…¨éƒ¨é‡‡é›†å®Œæˆ | æ€»æ•°æ®é‡: {total_count} æ¡")

    def generate_search_urls(self):
        """ç”Ÿæˆå°çº¢ä¹¦æœç´¢URL"""
        urls = []
        base_url = "https://www.xiaohongshu.com/search_result"
        
        for keyword in self.keyword_tool.generate():
            # å°çº¢ä¹¦æœç´¢å‚æ•°
            params = {
                "keyword": quote(keyword),
                "source": "web_explore_feed",
                "type": "note"  # åªæœç´¢ç¬”è®°ç±»å‹
            }
            
            # æ¯é¡µ20æ¡ç»“æœï¼Œçˆ¬å–5é¡µ
            for page in range(1, 6):
                params["page"] = page
                query_str = "&".join([f"{k}={v}" for k, v in params.items()])
                urls.append(f"{base_url}?{query_str}")
        
        logger.info(f"ç”Ÿæˆ {len(urls)} ä¸ªå°çº¢ä¹¦æœç´¢URL")
        return urls

    def parse_list_page(self, html: str):
        """è§£æå°çº¢ä¹¦æœç´¢ç»“æœé¡µ"""
        soup = BeautifulSoup(html, "html.parser")
        
        # å°çº¢ä¹¦æœç´¢ç»“æœç»“æ„å¯èƒ½æœ‰å¤šç§å½¢å¼ï¼Œå°è¯•ä¸¤ç§è§£ææ–¹å¼
        note_list = soup.select('div.note-item') or soup.select('div.card')
        
        if not note_list:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ç¬”è®°å¡ç‰‡ï¼Œå¯èƒ½é¡µé¢ç»“æ„å·²å˜æ›´")
            return
        
        for note in note_list:
            item = {}
            try:
                # ==== æ ‡é¢˜ä¸å†…å®¹ ====
                title_elem = note.select_one('.title, .content-title')
                content_elem = note.select_one('.desc, .content')
                
                item['title'] = title_elem.text.strip() if title_elem else "æ— æ ‡é¢˜"
                item['content'] = content_elem.text.strip() if content_elem else ""
                
                # ==== ä½œè€…ä¿¡æ¯ ====
                author_elem = note.select_one('.user-name, .author-info .name')
                item['author'] = author_elem.text.strip() if author_elem else "åŒ¿åç”¨æˆ·"
                
                # ==== äº’åŠ¨æ•°æ® ====
                interactions = note.select('.interaction-item, .counts span')
                item['likes'] = interactions[0].text.strip() if len(interactions) > 0 else "0"
                item['collects'] = interactions[1].text.strip() if len(interactions) > 1 else "0"
                item['comments'] = interactions[2].text.strip() if len(interactions) > 2 else "0"
                
                # ==== æ—¶é—´ä¿¡æ¯ ====
                time_elem = note.select_one('.time, .date')
                item['raw_post_time'] = time_elem.text.strip() if time_elem else "æ—¶é—´æœªæ ‡æ³¨"
                
                # ==== è¯¦æƒ…é“¾æ¥ ====
                link_elem = note.select_one('a[href^="/explore/"]')
                if link_elem and link_elem.has_attr('href'):
                    item['detail_url'] = f"{self.base_url}{link_elem['href']}"
                else:
                    item['detail_url'] = "é“¾æ¥æ— æ•ˆ"
                
                # ==== æœºæ„ç›¸å…³å…ƒæ•°æ® ====
                item.update({
                    'source': 'xiaohongshu',
                    'institution_name': self.institution_name,
                    'org_code': self.org_code,
                    'user_id': self.user_id
                })
                
                # æƒ…æ„Ÿåˆ†æ
                sentiment_result = self.sentiment_analyzer.analyze(
                    f"{item['title']} {item['content']}"
                )
                item['sentiment'] = sentiment_result[0]
                item['sentiment_score'] = sentiment_result[1]
                
                # æœ‰æ•ˆæ€§æ ¡éªŒ
                required_keyword = 'æ ‘äºº'
                valid_check = [
                    item.get('detail_url') and item['detail_url'] != "é“¾æ¥æ— æ•ˆ",
                    len(item.get('content', '')) > 10,
                    required_keyword in item.get('title', '') or required_keyword in item.get('content', '')
                ]
                
                if not all(valid_check):
                    reasons = []
                    if not valid_check[0]: reasons.append("æ— æ•ˆé“¾æ¥")
                    if not valid_check[1]: reasons.append(f"å†…å®¹è¿‡çŸ­({len(item.get('content',''))}å­—)")
                    if not valid_check[2]: reasons.append(f"æœªå«å…³é”®è¯'{required_keyword}'")
                    logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®: {', '.join(reasons)}")
                    continue
                
                self.data.append(item)
                
            except Exception as e:
                logger.error(f"è§£æå¼‚å¸¸: {str(e)}")
                # ä¿å­˜é”™è¯¯æ ·æœ¬ç”¨äºè°ƒè¯•
                with open("error_note.html", "w", encoding="utf-8") as f:
                    f.write(str(note))

    def _save_data(self):
        """å­˜å‚¨æ•°æ®åˆ°Supabase"""
        if not self.data:
            logger.warning("âš ï¸ æ— æœ‰æ•ˆæ•°æ®å¯å­˜å‚¨")
            return
        
        try:
            # æ‰¹é‡æ’å…¥å¹¶å»é‡
            response = self.supabase.table('posts').upsert(
                self.data,
                on_conflict='detail_url'
            ).execute()
            
            if response.data and len(response.data) > 0:
                logger.info(f"âœ… æˆåŠŸå†™å…¥ {len(response.data)} æ¡æ•°æ®")
            else:
                logger.warning("âš ï¸ æ— æ–°æ•°æ®å†™å…¥")
        except Exception as e:
            logger.error(f"å­˜å‚¨å¤±è´¥: {str(e)}")

class WeightedSentimentAnalyzer:
    """å°çº¢ä¹¦ä¸“ç”¨æƒ…æ„Ÿåˆ†æå™¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        # è´Ÿé¢è¯æƒé‡ï¼ˆé’ˆå¯¹æ•™è‚²é¢†åŸŸä¼˜åŒ–ï¼‰
        self.negative_weights = {
            "å·®è¯„": -4, "å¤±æœ›": -3, "é¿é›·": -4, "åæ§½": -3, 
            "å‘": -3, "ä¸æ¨è": -3, "åæ‚”": -3, "å·®åŠ²": -4,
            "ä¸è¡Œ": -2, "ä¸å¥½": -2, "åƒåœ¾": -5, "æŠ•è¯‰": -4
        }
        
        # æ­£é¢è¯æƒé‡
        self.positive_weights = {
            "æ¨è": +3, "æ»¡æ„": +3, "ç‚¹èµ": +3, "ä¼˜ç§€": +4,
            "ä¸“ä¸š": +4, "è¶…èµ": +4, "å€¼å¾—": +3, "å–œæ¬¢": +2,
            "å¥½": +1, "æ£’": +2, "å¼º": +2, "å‰å®³": +3
        }
        
        # æ•™è‚²é¢†åŸŸç‰¹å®šè¯
        self.education_weights = {
            "å¸ˆèµ„": +3, "æ•™å­¦": +3, "å­¦ä¹ æ°›å›´": +4, "å®è·µæœºä¼š": +3,
            "å°±ä¸š": +4, "è€ƒç ”": +3, "å¸ˆèµ„åŠ›é‡": +4, "æ ¡å›­ç¯å¢ƒ": +2,
            "è¯¾ç¨‹": +2, "ä¸“ä¸šè®¾ç½®": +3, "å­¦è´¹": -2, "å®¿èˆæ¡ä»¶": +1
        }
        
        self.thresholds = (-3, 3)  # (negative_threshold, positive_threshold)

    def calculate_score(self, text):
        """åŠ æƒè¯„åˆ†ç®—æ³•ï¼ˆæ•™è‚²é¢†åŸŸä¼˜åŒ–ç‰ˆï¼‰"""
        score = 0
        
        # è´Ÿé¢è¯æ£€æµ‹
        for word, weight in self.negative_weights.items():
            if word in text:
                score += weight * text.count(word)
                
        # æ­£é¢è¯æ£€æµ‹
        for word, weight in self.positive_weights.items():
            if word in text:
                score += weight * text.count(word)
                
        # æ•™è‚²é¢†åŸŸå…³é”®è¯æ£€æµ‹
        for word, weight in self.education_weights.items():
            if word in text:
                score += weight * text.count(word)
                
        # è¡¨æƒ…ç¬¦å·å¢å¼º
        if "ğŸ‘" in text or "â¤ï¸" in text:
            score += 2
        if "ğŸ‘" in text or "ğŸ’”" in text:
            score -= 2
            
        return score

    def analyze(self, text):
        """æƒ…æ„Ÿåˆ†ç±»"""
        score = self.calculate_score(text)
        if score <= self.thresholds[0]:
            return ("negative", score)
        elif score >= self.thresholds[1]:
            return ("positive", score)
        else:
            return ("neutral", score)

if __name__ == '__main__':
    import random
    load_dotenv()
    
    # æµ‹è¯•æƒ…æ„Ÿåˆ†æå™¨
    analyzer = WeightedSentimentAnalyzer()
    test_cases = [
        "æµ™æ±Ÿæ ‘äººçš„å¸ˆèµ„åŠ›é‡çœŸçš„è¶…æ£’ï¼è€å¸ˆéƒ½å¾ˆä¸“ä¸šï¼Œå­¦ä¹ æ°›å›´ä¹Ÿå¾ˆå¥½ ğŸ‘",
        "åæ‚”é€‰æ‹©äº†æ ‘å…°ï¼Œå®¿èˆæ¡ä»¶å¤ªå·®äº†ï¼Œé¿é›·ï¼",
        "æ ¡å›­ç¯å¢ƒä¸€èˆ¬ï¼Œä½†æ•™å­¦è¿˜å¯ä»¥ï¼Œè€ƒç ”ç‡ä¸é”™",
        "åƒåœ¾å­¦æ ¡ï¼Œåƒä¸‡åˆ«æ¥ï¼ğŸ‘"
    ]
    
    for text in test_cases:
        sentiment, score = analyzer.analyze(text)
        print(f"æ–‡æœ¬: {text}")
        print(f"æƒ…æ„Ÿ: {sentiment}, åˆ†æ•°: {score}")
        print("-" * 50)
    
    # è¿è¡Œçˆ¬è™«
    spider = XiaohongshuSpider(kw='æµ™æ±Ÿæ ‘äºº')
    try:
        spider.run()
    except Exception as e:
        logger.error(f"çˆ¬è™«å¼‚å¸¸ç»ˆæ­¢: {str(e)}")