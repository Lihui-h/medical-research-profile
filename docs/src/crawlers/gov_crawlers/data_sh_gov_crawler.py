# src/crawlers/gov_crawlers/data_sh_gov_crawler.py
import os
import json
import logging
import requests
import time
import random
import sys
from pymongo import MongoClient, UpdateOne
from pymongo.errors import BulkWriteError, ConnectionFailure
from dotenv import load_dotenv
from datetime import datetime
from pathlib import Path
from urllib.parse import quote
from .base_gov_crawler import BaseGovCrawler

class DataShGovCrawler(BaseGovCrawler):
    """ä¸Šæµ·åŒ»ä¿æ•°æ®é‡‡é›†ï¼ˆç»ˆæç¨³å®šç‰ˆï¼‰"""
    
    def __init__(self):
        super().__init__()
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        self.mongodb_uri = os.getenv("MONGODB_URI")
        self.db_name = os.getenv("MONGODB_DB", "gov_data")
        self.collection_name = os.getenv("MONGODB_COLLECTION", "sh_hospitals")

        # ä»ç¯å¢ƒå˜é‡è¯»å–å¤šä¸ªarea_id
        self.area_ids = list(map(
            str.strip,
            os.getenv("DEFAULT_AREA_IDS", "20").split(",")
        ))
        self.logger.info(f"åˆå§‹åŒ–é‡‡é›†åŒºåŸŸ: {self.area_ids}")

        # ç¡®ä¿ç¯å¢ƒå˜é‡å­˜åœ¨
        if not self.mongodb_uri:
            raise ValueError("MONGODB_URI æœªè®¾ç½®")

        # è¿æ¥ MongoDB Atlas
        self.client = MongoClient(self.mongodb_uri)
        # è¿æ¥åˆ°æ”¿åºœä¸“ç”¨æ•°æ®åº“
        self.gov_db = self.client[os.getenv("GOV_DB", "gov_data")]
        self.collection = self.gov_db[os.getenv("GOV_COLLECTION", "sh_hospitals")]

        # åˆå§‹åŒ–API URL
        self.api_url = "https://data.sh.gov.cn/interface/AG9102015009/14661"
        self.proxy_enabled = False  # å¼ºåˆ¶ç¦ç”¨ä»£ç†
        # åˆå§‹åŒ–è¯·æ±‚å‚æ•°ï¼ˆåŒé‡ç¼–ç ä¿æŠ¤ï¼‰
        self.required_params = {
            "area_id": "default_value",  # å ä½ç¬¦
            "limit": 200,
            "offset": 0
        }

    def crawl_medical_institutions(self):
        """ä¸»é‡‡é›†æµç¨‹"""
        total_records = 0
        MAX_RETRIES = 3  # å•é¡µè¯·æ±‚é‡è¯•æ¬¡æ•°
        REQUEST_DELAY = 1  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰

        # éå†æ‰€æœ‰area_id
        for area_id in self.area_ids:
            self.logger.info(f"å¼€å§‹é‡‡é›†åŒºåŸŸ: area_id={area_id}")
            current_offset = 0  # é‡ç½®offset
            has_more = True
            retry_count = 0

            while has_more:
                # æ„å»ºå½“å‰åŒºåŸŸçš„è¯·æ±‚å‚æ•°
                params = {
                    "area_id": str(area_id),
                    "limit": 200,
                    "offset": current_offset
                }

                # è¯·æ±‚æ•°æ®ï¼ˆå«é‡è¯•æœºåˆ¶ï¼‰
                data = []
                for attempt in range(MAX_RETRIES):
                    try:
                        self.logger.debug(f"â–· è¯·æ±‚å‚æ•°: {params}")
                        result = self.safe_api_request(params)

                        if not result or "data" not in result:
                            self.logger.warning(f"ç¬¬ {attempt+1} æ¬¡é‡è¯•...")
                            continue
                        data = result["data"].get("data", [])
                        actual_count = len(data)
                        self.logger.info(f"âœ” è·å–åˆ° {actual_count} æ¡æ•°æ®")
                        break  # æˆåŠŸè·å–æ•°æ®ï¼Œé€€å‡ºé‡è¯•å¾ªç¯
                    except Exception as e:
                        self.logger.error(f"è¯·æ±‚å¼‚å¸¸: {str(e)}")
                        time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                else:
                    self.logger.error(f"â— åŒºåŸŸ {area_id} åˆ†é¡µç»ˆæ­¢ï¼Œè¿ç»­ {MAX_RETRIES} æ¬¡è¯·æ±‚å¤±è´¥")
                    has_more = False
                    continue
                # ä¿å­˜æ•°æ®
                if data:
                    if self.save_data(data, "gov_reports", f"medical_{area_id}.json"):
                        total_records += len(data)

                    # åˆ†é¡µç»ˆæ­¢æ¡ä»¶ï¼šå®é™…è¿”å›æ•°é‡å°äºlimit
                    if len(data) < params["limit"]:
                        self.logger.info(f"âœ… åŒºåŸŸ {area_id} æ•°æ®é‡‡é›†å®Œæˆï¼Œå…± {current_offset + len(data)} æ¡")
                        has_more = False
                    else:
                        current_offset += params["limit"]
                else:
                    self.logger.info(f"âœ… åŒºåŸŸ {area_id} æ— æ›´å¤šæ•°æ®")
                    has_more = False
                
                # è¯·æ±‚é—´éš”ï¼ˆè§„é¿åçˆ¬ï¼‰
                time.sleep(REQUEST_DELAY + random.uniform(0, 1))

        self.logger.info(f"ğŸ å…¨éƒ¨é‡‡é›†å®Œæˆ | æ€»æ•°æ®é‡: {total_records} æ¡")

    def safe_api_request(self, params, max_retries=3):
        """APIè¯·æ±‚å°è£…ï¼ˆå«ç¼–ç éªŒè¯ï¼‰"""
        for attempt in range(max_retries):
            try:
                response = self.safe_request(
                    method="POST",
                    url=self.api_url,
                    json=params
                )
                
                if not response or response.status_code != 200:
                    self.logger.warning(f"æ— æ•ˆå“åº” | çŠ¶æ€ç : {getattr(response, 'status_code', 'æ— ')}")
                    continue
                
                # è§£æå¤–å±‚JSON
                raw_data = response.json()
                self.logger.debug(f"å¤–å±‚è§£æç»“æœ: {json.dumps(raw_data, ensure_ascii=False, indent=2)}")

                # å¤„ç†åµŒå¥—çš„JSONå­—ç¬¦ä¸²
                if isinstance(raw_data.get("data"), str):
                    try:
                        inner_data = json.loads(raw_data["data"])
                        raw_data["data"] = inner_data
                        self.logger.debug(f"å†…å±‚è§£æç»“æœ: {json.dumps(inner_data, ensure_ascii=False, indent=2)}")
                        
                    except Exception as e:
                        self.logger.error(f"è§£æåµŒå¥—JSONå¤±è´¥: {str(e)}")
                        continue
                # æå–æœ‰æ•ˆæ•°æ®
                data_list = raw_data.get("data", {}).get("data", [])
                total = raw_data.get("data", {}).get("total", 0)
                
                return {"data": {"data": data_list, "total": total}}
                
            except Exception as e:
                self.logger.error(f"è¯·æ±‚å¼‚å¸¸ï¼ˆç¬¬{attempt+1}æ¬¡é‡è¯•ï¼‰: {str(e)}")
                time.sleep(2 ** attempt)
        return None

    def save_data(self, data, sub_dir, file_name):
        """å¢å¼ºç‰ˆMongoDBå­˜å‚¨æ–¹æ³•ï¼ˆæ”¯æŒæ‰¹é‡æ“ä½œ/æ–­è¿é‡è¯•/æ™ºèƒ½å»é‡ï¼‰"""
        try:
            # ================== è¿‡æ»¤ä¸å»é‡é€»è¾‘ ==================
            exclude_keywords = ["åŠ©è€æœåŠ¡ç¤¾", "æŠ¤ç†é™¢", "å«ç”Ÿå®¤", "å…»è€é™¢", "æ•¬è€é™¢"]
            seen = set()  # ç”¨äºå»é‡çš„é›†åˆ
            filtered_data = []

            for item in data:
                # æ’é™¤æ— æ•ˆæœºæ„ç±»å‹
                if any(kw in item.get("name", "") for kw in exclude_keywords):
                    self.logger.warning(f"è¿‡æ»¤æ— æ•ˆæœºæ„: {item.get('name')}")
                    continue

                # æ ‡å‡†åŒ–åç§°å’Œåœ°å€
                name = item.get("name", "").strip().lower()
                address = item.get("address", "").strip().lower()
                
                # è·³è¿‡æ— æ•ˆæ•°æ®ï¼ˆå¯é€‰æ‰©å±•æ ¡éªŒé€»è¾‘ï¼‰
                if not name or not address:
                    self.logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®: {item}")
                    continue

                # å»é‡æ ‡è¯†ç¬¦ï¼ˆåç§°+åœ°å€ï¼‰
                identifier = f"{name}_{address}"
                if identifier in seen:
                    self.logger.warning(f"æ£€æµ‹åˆ°é‡å¤æ•°æ®: {name}")
                    continue
                seen.add(identifier)

                # ä¿ç•™æœ‰æ•ˆæ•°æ®
                filtered_data.append({
                    **item,
                    "name": name,
                    "address": address
                })

            # æ›´æ–°æ•°æ®ä¸ºè¿‡æ»¤åç»“æœ
            data = filtered_data
            self.logger.info(f"è¿‡æ»¤åå‰©ä½™æœ‰æ•ˆæ•°æ®: {len(data)}æ¡")

            # ================== åŸæœ‰MongoDBå­˜å‚¨é€»è¾‘ ==================
            operations = []
            for item in data:
                operation = UpdateOne(
                    {"name": item["name"], "address": item["address"]},
                    {"$set": item},
                    upsert=True
                )
                operations.append(operation)
            # ================== æ‰¹é‡å†™å…¥ ==================
            if operations:
                try:
                    result = self.collection.bulk_write(
                        operations,
                        ordered=False  # æ— åºå†™å…¥æå‡æ€§èƒ½
                    )
                    duplicate_count = len(operations) - (result.upserted_count + result.modified_count)
                    
                    self.logger.info(
                        f"MongoDBå†™å…¥ç»“æœ | æ–°å¢: {result.upserted_count} "
                        f"æ›´æ–°: {result.modified_count} "
                        f"é‡å¤: {duplicate_count}"
                    )
                    
                except BulkWriteError as bwe:
                    # å¤„ç†éƒ¨åˆ†é‡å¤å†™å…¥çš„æƒ…å†µ
                    duplicate_count = len(bwe.details['writeErrors'])
                    self.logger.warning(
                        f"æ‰¹é‡å†™å…¥éƒ¨åˆ†æˆåŠŸ | å·²æ’å…¥: {bwe.details['nInserted']} "
                        f"é‡å¤é¡¹: {duplicate_count}"
                    )

            # ================== æ•°æ®å¤‡ä»½æœºåˆ¶ ==================
            # ä¿ç•™åŸå§‹JSONå¤‡ä»½ï¼ˆå¯é€‰ï¼‰
            backup_path = self.BASE_DIR / "data/backup" / sub_dir / file_name
            backup_path.parent.mkdir(parents=True, exist_ok=True)
            with open(backup_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            self.logger.debug(f"åŸå§‹æ•°æ®å·²å¤‡ä»½è‡³: {backup_path}")

            return True

        except KeyError as e:
            self.logger.error(f"æ•°æ®å­—æ®µç¼ºå¤±: {str(e)}", exc_info=True)
            return False
        except Exception as e:
            self.logger.error(f"æ•°æ®åº“æ“ä½œå¼‚å¸¸: {str(e)}", exc_info=True)
            return False



if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler()]
    )
    crawler = DataShGovCrawler()
    crawler.crawl_medical_institutions()
